{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fd6880ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\ishan\\Automation\\SIH25\\RAG\")  # So we can import rag_script\n",
    "\n",
    "from RAG import rag_query\n",
    "from RAG import vector_store\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "\n",
    "\n",
    "# SQLite DB file\n",
    "DB_FILE = Path(\"conversation_memory.db\")\n",
    "\n",
    "# Initialize SQLite connection\n",
    "conn = sqlite3.connect(DB_FILE, check_same_thread=False)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create conversation table if not exists\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS conversation_history (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    user_id TEXT NOT NULL,\n",
    "    role TEXT NOT NULL,  -- 'user' or 'assistant'\n",
    "    text TEXT NOT NULL,\n",
    "    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    ")\n",
    "\"\"\")\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b788195e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "26b87640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM (Google Gemini)\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "\n",
    "# Prompt template\n",
    "template = \"\"\"\n",
    "You are an academic assistant. Answer in the language specified.\n",
    "Use the context to answer the question accurately.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e9c002fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conversation_history(user_id, last_n=5):\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT role, text FROM conversation_history\n",
    "        WHERE user_id = ?\n",
    "        ORDER BY timestamp DESC\n",
    "        LIMIT ?\n",
    "    \"\"\", (user_id, last_n))\n",
    "    rows = cursor.fetchall()\n",
    "    # Reverse to chronological order\n",
    "    return [{\"role\": r[0], \"text\": r[1]} for r in reversed(rows)]\n",
    "\n",
    "def save_turn(user_id, role, text):\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO conversation_history (user_id, role, text)\n",
    "        VALUES (?, ?, ?)\n",
    "    \"\"\", (user_id, role, text))\n",
    "    conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b1848006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rg_generate(user_id, rag_output, language=\"en\"):\n",
    "    # Context is already a list of strings from rag_query\n",
    "    context = \"\\n\".join(rag_output[\"context\"])\n",
    "    question = rag_output[\"query\"]\n",
    "\n",
    "    # Updated prompt (forces grounding to docs)\n",
    "    final_prompt = f\"\"\"\n",
    "You are an academic assistant. \n",
    "Answer the question as much as possible with the given context and if the context was used state where it was mentioned and if it was not used then state clearly   . \n",
    "\"\n",
    "\n",
    "Answer in {language}.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "    # Call LLM properly\n",
    "    from langchain.schema import HumanMessage\n",
    "    response = llm.invoke([HumanMessage(content=final_prompt)])\n",
    "    response_text = response.content\n",
    "\n",
    "    # Save conversation to DB\n",
    "    save_turn(user_id, \"user\", question)\n",
    "    save_turn(user_id, \"assistant\", response_text)\n",
    "\n",
    "    # Extract sources from metadata\n",
    "    sources = [meta.get(\"source\", \"N/A\") for meta in rag_output[\"metadata\"]]\n",
    "    return response_text, sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ca56cb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé RAG Output Debug:\n",
      "Query: tell me the syllabus of Software Engineering subject.\n",
      "Context count: 0\n",
      "\n",
      "Answer:\n",
      " The provided context is empty. Therefore, I cannot tell you the syllabus of the Software Engineering subject based on the given context.\n",
      "\n",
      "Sources:\n",
      " []\n"
     ]
    }
   ],
   "source": [
    "user_id = \"user_123\"\n",
    "user_query = \"tell me the syllabus of Software Engineering subject.\"\n",
    "\n",
    "# 1Ô∏è‚É£ Retrieve RAG output\n",
    "rag_output = rag_query(user_query, retriever)\n",
    "\n",
    "print(\"üîé RAG Output Debug:\")\n",
    "print(\"Query:\", rag_output[\"query\"])\n",
    "print(\"Context count:\", len(rag_output[\"context\"]))\n",
    "for i, ctx in enumerate(rag_output[\"context\"], 1):\n",
    "    print(f\"\\n--- Context {i} ---\\n{ctx[:500]}\")\n",
    "\n",
    "# 2Ô∏è‚É£ Generate response\n",
    "answer, sources = rg_generate(user_id, rag_output, language=\"en\")\n",
    "\n",
    "print(\"\\nAnswer:\\n\", answer)\n",
    "print(\"\\nSources:\\n\", sources)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
